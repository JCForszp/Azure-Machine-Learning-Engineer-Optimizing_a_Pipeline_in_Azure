# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

The aim of this project is to apply two methods to determine the model with the best accuracy to predict whether a customer will or won't subscribe to a given bank service (classification).  
- The first method used is to fix the model, in our case a logistic regression, and let Azure HyperDrive determine the best parameters.
- The second method is to apply Azure AutoML to the same dataset and let it determine the model and parameters that give the best accuracy. 

The project uses a unique dataset made out of 32 950 observations - with some caveats: 
three features are highly correlated (emp.var.rate, euribor3m and nr.employed) and the outcomes are largely imbalanced 
(88.6% rejected the offer, 11.2% accepted it, leading to a null accuracy of almost 89% from the start). 
So, the gap between the accuracy of the first method (91.76%) with the second (91.68%) and the null accuracy (88.6%) is relatively small. 
[Analysis](bankmarketing.html). More work needs to be done on the dataset to provide more positive cases. 

Relying exclusively on the accuracy, the best performing model was the logistics regression, at the end. 

# Details of the setups of the 2 methods

## Method 1 - Logistics Regression (Scikit-learn Pipeline)
**Pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The classification algorithm was provided for this part of the project. Most of the 21 features were numerical, the few categorical ones could be easily be one-hot-encoded and the targets were binary ones (0 for rejection, 1 for acceptance). So, a logistics regression model looks well appropriate for such classification task with this size and structure of the dataset. Dealing with a classification, the primary metric that makes most sense in that case is the Accuracy - a criteria that shall be maximized. 

The pipeline architecture was split into a python script, that contains the data cleanup, the split into train & test datasets as well as the logistics regression model itself. 
The script uses a parser to define as arguments the two hyperparameters required by the model: 
- one for the regularization strength and 
- one for the maximum number of iterations. 
This will allow to be called repetitively from the notebook, by HyperDrive, during the search for optimal parameters.

**Benefits of the parameter sampler:**
The pipeline is driven from the Jupyter notebook udacity-project.ipynb that defines the context for the search of the optimal hyperparameters (within that context).
Hyperparameter space: I chose the RandomParameterSampling, mainly for speed reason, as the usual alternative, GridParameterSampling, would have triggered an exhaustive search over the complete space, for a gain that proved to be relatively small at the end. Also, GridParameterSampling only allows discrete values, while random sampling is more open as it allows also the use of continuous values. 

**Benefits of the early stopping policy:**
Regarding early termination of poorly performing runs, I used the BanditPolicy. The BanditPolicy defines a slack factor (defined here to 0.1). All runs that fall outside the slack factor with respect to the best performing run will be terminated, saving time and budget. 

**Tuned hyperparameters and accuracy obtained:**
```
{'Regularization Strength:': 1.1777921298523928, 'Max iterations:': 50, 'Accuracy': 0.9176024279210926}
```

## Method 2 - AutoML
**Model and hyperparameters generated by AutoML.**

[Best Model Details](BestModelDetails.png)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
